# ğŸ¯ JARVIS Quick Reference

## Quick Start Commands

```powershell
# 1. Activate virtual environment
.\venv\Scripts\Activate.ps1

# 2. Run JARVIS
streamlit run app.py

# 3. Check system health
python utils.py

# 4. Interactive chat (no UI)
python example_usage.py
```

## Project Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER                                 â”‚
â”‚                    (Conversations)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    JARVIS (app.py)                           â”‚
â”‚                  Streamlit Chat UI                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â”‚ Upload Files                    â”‚ Ask Questions
            â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Document Ingestion â”‚            â”‚    RAG Assistant         â”‚
â”‚   (ingestion.py)    â”‚            â”‚  (rag_assistant.py)      â”‚
â”‚                     â”‚            â”‚                          â”‚
â”‚ â€¢ Read files        â”‚            â”‚ â€¢ Retrieve context       â”‚
â”‚ â€¢ Chunk text        â”‚            â”‚ â€¢ Generate answers       â”‚
â”‚ â€¢ Create embeddings â”‚            â”‚ â€¢ Format responses       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                               â”‚
           â”‚ Store                         â”‚ Query
           â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Vector Database (Pinecone)                     â”‚
â”‚          â€¢ Stores document embeddings                        â”‚
â”‚          â€¢ Fast semantic search                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â”‚ Retrieved Context
                                           â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚   LLM (llm_handler)  â”‚
                                â”‚   â€¢ Ollama (LLaMA2)  â”‚
                                â”‚   â€¢ OpenAI (backup)  â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## File Overview

| File | Purpose |
|------|---------|
| `app.py` | Main Streamlit UI application |
| `config.py` | Configuration settings |
| `ingestion.py` | Document upload & processing |
| `llm_handler.py` | LLM integration (Ollama/OpenAI) |
| `rag_assistant.py` | RAG logic & chat handling |
| `utils.py` | Utility functions & health checks |
| `example_usage.py` | Command-line usage examples |
| `requirements.txt` | Python dependencies |
| `.env` | Environment variables (YOU CREATE THIS) |

## Environment Variables

```env
# Required
PINECONE_API_KEY=your_api_key_here
PINECONE_ENVIRONMENT=us-east-1
PINECONE_INDEX_NAME=jarvis-assistant

# LLM (choose one)
OLLAMA_MODEL=llama2                    # For local LLaMA
# OR
OPENAI_API_KEY=your_openai_key_here   # For cloud OpenAI
```

## Key Configuration (config.py)

```python
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
CHUNK_SIZE = 1000          # Size of text chunks
CHUNK_OVERLAP = 200        # Overlap between chunks
TOP_K_RESULTS = 3          # Number of results to retrieve
```

## Supported File Types

- âœ… `.txt` - Plain text files
- âœ… `.pdf` - PDF documents
- âœ… `.docx` - Word documents

## Common Tasks

### Add Documents
```python
from ingestion import DocumentIngestion

ingest = DocumentIngestion()
ingest.ingest_file("path/to/file.pdf", "my_document.pdf")
```

### Query Programmatically
```python
from rag_assistant import RAGAssistant

assistant = RAGAssistant()
response = assistant.chat("What are my projects?")
print(response['answer'])
```

### Check System Status
```powershell
python utils.py
```

## Streamlit UI Features

### Sidebar
- ğŸ“ File upload
- âš™ï¸ System status
- ğŸ—‘ï¸ Clear chat
- â„¹ï¸ About section

### Main Chat
- ğŸ’¬ Conversational interface
- ğŸ“š Source attribution
- ğŸ”„ Real-time responses

## LLM Models

### Ollama Models (Local)
```powershell
ollama pull llama2      # Recommended, balanced
ollama pull phi         # Fastest, lighter
ollama pull mistral     # More capable
ollama pull codellama   # Best for code
```

### Model Comparison
| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| phi | 2.7GB | âš¡âš¡âš¡ | â­â­ |
| llama2 | 3.8GB | âš¡âš¡ | â­â­â­ |
| mistral | 4.1GB | âš¡âš¡ | â­â­â­â­ |
| llama2:13b | 7.3GB | âš¡ | â­â­â­â­â­ |

## Performance Tips

### Faster Responses
1. Use smaller model: `phi` or `llama2`
2. Reduce `TOP_K_RESULTS` in config
3. Use shorter documents
4. Consider OpenAI API

### Better Quality
1. Use larger model: `mistral` or `llama2:13b`
2. Increase `CHUNK_OVERLAP` for better context
3. Upload well-structured documents
4. Ask more specific questions

## Troubleshooting Checklist

- [ ] Virtual environment activated?
- [ ] All packages installed? (`pip list`)
- [ ] `.env` file created and configured?
- [ ] Pinecone API key valid?
- [ ] Ollama installed and running?
- [ ] LLaMA model downloaded?
- [ ] Documents uploaded and processed?
- [ ] Internet connection active?

## Useful Commands

```powershell
# Check what's running
Get-Process | Where-Object {$_.ProcessName -like "*ollama*"}

# Stop Streamlit
Ctrl + C (in the terminal)

# Restart with fresh cache
streamlit run app.py --server.headless true

# Check Ollama models
ollama list

# Test Ollama
ollama run llama2 "Hello"

# View uploaded files
ls uploaded_files

# View Pinecone stats
python -c "from utils import get_index_stats; get_index_stats()"
```

## Example Queries

### Personal Information
- "What is my name?"
- "What are my skills?"
- "What projects am I working on?"

### Specific Topics
- "Tell me about my AI projects"
- "What books do I like?"
- "What are my goals for 2025?"

### Summaries
- "Summarize my notes about machine learning"
- "What are the key points in my documents?"
- "Give me an overview of my interests"

## API Usage (Advanced)

```python
# Custom chunk size
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)

# Custom retrieval
assistant = RAGAssistant()
contexts = assistant.retrieve_context("query", top_k=5)

# Direct LLM access
from llm_handler import LLMHandler

llm = LLMHandler()
response = llm.generate_response("Your prompt here")
```

## Resources

### Official Documentation
- Streamlit: https://docs.streamlit.io/
- LangChain: https://python.langchain.com/
- Pinecone: https://docs.pinecone.io/
- Ollama: https://ollama.ai/

### Learning Materials
- RAG Tutorial: Search "Retrieval Augmented Generation tutorial"
- Vector Databases: Pinecone learning center
- LLaMA Guide: Ollama documentation

### Community
- Streamlit Forum: https://discuss.streamlit.io/
- LangChain Discord: Via langchain.com
- GitHub Issues: Create in your repo

## Development Workflow

1. **Setup** (one time)
   - Install dependencies
   - Configure environment
   - Set up Pinecone & Ollama

2. **Daily Use**
   - Activate venv
   - Run `streamlit run app.py`
   - Upload documents
   - Chat with JARVIS

3. **Maintenance**
   - Update dependencies occasionally
   - Clear old uploaded files
   - Monitor Pinecone usage

---

## ğŸ‰ You're Ready!

Start JARVIS with:
```powershell
streamlit run app.py
```

Happy chatting! ğŸ¤–
